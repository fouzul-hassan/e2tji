{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ZuCo 1.0 & 2.0 EDA (Exploratory Data Analysis)\n",
    "\n",
    "This notebook explores the ZuCo datasets for EEG-to-Text research.\n",
    "\n",
    "**Datasets:**\n",
    "- **ZuCo 1.0**: 12 subjects (Z prefix), 3 tasks\n",
    "- **ZuCo 2.0**: 18 subjects (Y prefix), 1 task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q matplotlib seaborn pandas numpy scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"Libraries loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update these paths to match your data location\n",
    "ZUCO1_PATHS = {\n",
    "    'task1-SR': 'C:\\\\MSc Files\\\\MSc Project\\\\E2T-w-VJEPA\\\\e2t-w-jepa-pretraining\\\\dataset\\\\ZuCo\\\\task1-SR\\\\pickle\\\\task1-SR-dataset-spectro.pickle',\n",
    "    'task2-NR': 'C:\\\\MSc Files\\\\MSc Project\\\\E2T-w-VJEPA\\\\e2t-w-jepa-pretraining\\\\dataset\\\\ZuCo\\\\task2-NR\\\\pickle\\\\task2-NR-dataset-spectro.pickle',\n",
    "    'task3-TSR': 'C:\\\\MSc Files\\\\MSc Project\\\\E2T-w-VJEPA\\\\e2t-w-jepa-pretraining\\\\dataset\\\\ZuCo\\\\task3-TSR\\\\pickle\\\\task3-TSR-dataset-spectro.pickle',\n",
    "}\n",
    "\n",
    "ZUCO2_PATHS = {\n",
    "    'task2-NR-2.0': 'C:\\\\MSc Files\\\\MSc Project\\\\E2T-w-VJEPA\\\\e2t-w-jepa-pretraining\\\\dataset\\\\ZuCo\\\\task2-NR-2.0\\\\pickle\\\\task2-NR-2.0-dataset-spectro.pickle',\n",
    "}\n",
    "\n",
    "def load_pickle(path):\n",
    "    \"\"\"Load pickle file.\"\"\"\n",
    "    if not Path(path).exists():\n",
    "        print(f\"âš ï¸ File not found: {path}\")\n",
    "        return None\n",
    "    print(f\"Loading {Path(path).name}...\")\n",
    "    with open(path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading task1-SR-dataset-spectro.pickle...\n",
      "  âœ“ task1-SR: 12 subjects\n",
      "Loading task2-NR-dataset-spectro.pickle...\n",
      "  âœ“ task2-NR: 12 subjects\n",
      "Loading task3-TSR-dataset-spectro.pickle...\n",
      "  âœ“ task3-TSR: 12 subjects\n",
      "Loading task2-NR-2.0-dataset-spectro.pickle...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m zuco2_data \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task_name, path \u001b[38;5;129;01min\u001b[39;00m ZUCO2_PATHS\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m---> 12\u001b[0m     data \u001b[38;5;241m=\u001b[39m load_pickle(path)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[0;32m     14\u001b[0m         zuco2_data[task_name] \u001b[38;5;241m=\u001b[39m data\n",
      "Cell \u001b[1;32mIn[7], line 19\u001b[0m, in \u001b[0;36mload_pickle\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPath(path)\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 19\u001b[0m     data \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\numpy\\core\\numeric.py:1851\u001b[0m, in \u001b[0;36m_frombuffer\u001b[1;34m(buf, dtype, shape, order)\u001b[0m\n\u001b[0;32m   1845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1848\u001b[0m _fromfunction_with_like \u001b[38;5;241m=\u001b[39m array_function_dispatch()(fromfunction)\n\u001b[1;32m-> 1851\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_frombuffer\u001b[39m(buf, dtype, shape, order):\n\u001b[0;32m   1852\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m frombuffer(buf, dtype\u001b[38;5;241m=\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mreshape(shape, order\u001b[38;5;241m=\u001b[39morder)\n\u001b[0;32m   1855\u001b[0m \u001b[38;5;129m@set_module\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1856\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21misscalar\u001b[39m(element):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load ZuCo 1.0\n",
    "zuco1_data = {}\n",
    "for task_name, path in ZUCO1_PATHS.items():\n",
    "    data = load_pickle(path)\n",
    "    if data:\n",
    "        zuco1_data[task_name] = data\n",
    "        print(f\"  âœ“ {task_name}: {len(data)} subjects\")\n",
    "\n",
    "# Load ZuCo 2.0\n",
    "zuco2_data = {}\n",
    "for task_name, path in ZUCO2_PATHS.items():\n",
    "    data = load_pickle(path)\n",
    "    if data:\n",
    "        zuco2_data[task_name] = data\n",
    "        print(f\"  âœ“ {task_name}: {len(data)} subjects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_stats(data, dataset_name):\n",
    "    \"\"\"Extract statistics from pickle data.\"\"\"\n",
    "    stats = {\n",
    "        'dataset': dataset_name,\n",
    "        'subjects': list(data.keys()),\n",
    "        'n_subjects': len(data),\n",
    "        'total_sentences': 0,\n",
    "        'sentences_per_subject': [],\n",
    "        'text_lengths': [],\n",
    "        'eeg_time_lengths': [],\n",
    "    }\n",
    "    \n",
    "    for subject_id, sentences in data.items():\n",
    "        n_sentences = len(sentences)\n",
    "        stats['total_sentences'] += n_sentences\n",
    "        stats['sentences_per_subject'].append(n_sentences)\n",
    "        \n",
    "        for sent in sentences:\n",
    "            if sent is None:\n",
    "                continue\n",
    "            # Text length\n",
    "            text = sent.get('content', '')\n",
    "            stats['text_lengths'].append(len(text.split()))\n",
    "            \n",
    "            # EEG time length\n",
    "            eeg_data = sent.get('sentence_level_EEG', {})\n",
    "            if 'rawData' in eeg_data:\n",
    "                raw = np.array(eeg_data['rawData'])\n",
    "                if raw.ndim == 2:\n",
    "                    stats['eeg_time_lengths'].append(raw.shape[1] if raw.shape[0] == 105 else raw.shape[0])\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Collect stats\n",
    "all_stats = []\n",
    "for task_name, data in zuco1_data.items():\n",
    "    all_stats.append(get_dataset_stats(data, f\"ZuCo1-{task_name}\"))\n",
    "for task_name, data in zuco2_data.items():\n",
    "    all_stats.append(get_dataset_stats(data, f\"ZuCo2-{task_name}\"))\n",
    "\n",
    "# Display summary\n",
    "summary_df = pd.DataFrame([{\n",
    "    'Dataset': s['dataset'],\n",
    "    'Subjects': s['n_subjects'],\n",
    "    'Total Sentences': s['total_sentences'],\n",
    "    'Avg Sentences/Subject': s['total_sentences'] / s['n_subjects'] if s['n_subjects'] > 0 else 0,\n",
    "    'Avg Words/Sentence': np.mean(s['text_lengths']) if s['text_lengths'] else 0,\n",
    "    'Avg EEG Time (samples)': np.mean(s['eeg_time_lengths']) if s['eeg_time_lengths'] else 0,\n",
    "} for s in all_stats])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\"*80)\n",
    "display(summary_df.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Subject Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZuCo 1.0 subjects\n",
    "zuco1_subjects = set()\n",
    "for data in zuco1_data.values():\n",
    "    zuco1_subjects.update(data.keys())\n",
    "\n",
    "# ZuCo 2.0 subjects\n",
    "zuco2_subjects = set()\n",
    "for data in zuco2_data.values():\n",
    "    zuco2_subjects.update(data.keys())\n",
    "\n",
    "print(f\"ZuCo 1.0 Subjects ({len(zuco1_subjects)}): {sorted(zuco1_subjects)}\")\n",
    "print(f\"ZuCo 2.0 Subjects ({len(zuco2_subjects)}): {sorted(zuco2_subjects)}\")\n",
    "print(f\"\\nTotal unique subjects: {len(zuco1_subjects | zuco2_subjects)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentences per subject per task\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ZuCo 1.0\n",
    "ax = axes[0]\n",
    "for task_name, data in zuco1_data.items():\n",
    "    subjects = list(data.keys())\n",
    "    counts = [len(data[s]) for s in subjects]\n",
    "    ax.bar([f\"{s}\" for s in subjects], counts, alpha=0.7, label=task_name)\n",
    "ax.set_xlabel('Subject')\n",
    "ax.set_ylabel('Number of Sentences')\n",
    "ax.set_title('ZuCo 1.0: Sentences per Subject')\n",
    "ax.legend()\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# ZuCo 2.0\n",
    "ax = axes[1]\n",
    "for task_name, data in zuco2_data.items():\n",
    "    subjects = list(data.keys())\n",
    "    counts = [len(data[s]) for s in subjects]\n",
    "    ax.bar(subjects, counts, alpha=0.7, label=task_name)\n",
    "ax.set_xlabel('Subject')\n",
    "ax.set_ylabel('Number of Sentences')\n",
    "ax.set_title('ZuCo 2.0: Sentences per Subject')\n",
    "ax.legend()\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_texts(data):\n",
    "    \"\"\"Collect all texts from data.\"\"\"\n",
    "    texts = []\n",
    "    for sentences in data.values():\n",
    "        for sent in sentences:\n",
    "            if sent and 'content' in sent:\n",
    "                texts.append(sent['content'])\n",
    "    return texts\n",
    "\n",
    "# Collect all texts\n",
    "all_texts = {'ZuCo 1.0': [], 'ZuCo 2.0': []}\n",
    "for data in zuco1_data.values():\n",
    "    all_texts['ZuCo 1.0'].extend(collect_texts(data))\n",
    "for data in zuco2_data.values():\n",
    "    all_texts['ZuCo 2.0'].extend(collect_texts(data))\n",
    "\n",
    "print(f\"ZuCo 1.0 sentences: {len(all_texts['ZuCo 1.0'])}\")\n",
    "print(f\"ZuCo 2.0 sentences: {len(all_texts['ZuCo 2.0'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for idx, (name, texts) in enumerate(all_texts.items()):\n",
    "    ax = axes[idx]\n",
    "    word_counts = [len(t.split()) for t in texts]\n",
    "    char_counts = [len(t) for t in texts]\n",
    "    \n",
    "    ax.hist(word_counts, bins=50, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "    ax.axvline(np.mean(word_counts), color='red', linestyle='--', label=f'Mean: {np.mean(word_counts):.1f}')\n",
    "    ax.axvline(np.median(word_counts), color='orange', linestyle='--', label=f'Median: {np.median(word_counts):.1f}')\n",
    "    ax.set_xlabel('Words per Sentence')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title(f'{name}: Text Length Distribution')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print stats\n",
    "for name, texts in all_texts.items():\n",
    "    word_counts = [len(t.split()) for t in texts]\n",
    "    print(f\"\\n{name} Text Stats:\")\n",
    "    print(f\"  Min words: {min(word_counts)}\")\n",
    "    print(f\"  Max words: {max(word_counts)}\")\n",
    "    print(f\"  Mean words: {np.mean(word_counts):.2f}\")\n",
    "    print(f\"  Std words: {np.std(word_counts):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample sentences\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE SENTENCES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, texts in all_texts.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    for i, text in enumerate(texts[:5]):\n",
    "        print(f\"  [{i+1}] {text[:100]}{'...' if len(text) > 100 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. EEG Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eeg_sample(data, n=1):\n",
    "    \"\"\"Get sample EEG data from dataset.\"\"\"\n",
    "    samples = []\n",
    "    for sentences in data.values():\n",
    "        for sent in sentences:\n",
    "            if sent is None:\n",
    "                continue\n",
    "            eeg_data = sent.get('sentence_level_EEG', {})\n",
    "            if 'rawData' in eeg_data:\n",
    "                raw = np.array(eeg_data['rawData'])\n",
    "                if raw.ndim == 2 and raw.shape[0] == 105:\n",
    "                    samples.append({\n",
    "                        'raw': raw,\n",
    "                        'text': sent.get('content', ''),\n",
    "                    })\n",
    "                    if len(samples) >= n:\n",
    "                        return samples\n",
    "    return samples\n",
    "\n",
    "# Get sample from first available dataset\n",
    "sample_data = None\n",
    "if zuco1_data:\n",
    "    first_task = list(zuco1_data.keys())[0]\n",
    "    sample_data = get_eeg_sample(zuco1_data[first_task], n=3)\n",
    "elif zuco2_data:\n",
    "    first_task = list(zuco2_data.keys())[0]\n",
    "    sample_data = get_eeg_sample(zuco2_data[first_task], n=3)\n",
    "\n",
    "if sample_data:\n",
    "    print(f\"Sample EEG shape: {sample_data[0]['raw'].shape}\")\n",
    "    print(f\"  Channels: {sample_data[0]['raw'].shape[0]}\")\n",
    "    print(f\"  Time samples: {sample_data[0]['raw'].shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize EEG data\n",
    "if sample_data:\n",
    "    fig, axes = plt.subplots(len(sample_data), 1, figsize=(14, 4*len(sample_data)))\n",
    "    if len(sample_data) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, sample in enumerate(sample_data):\n",
    "        ax = axes[idx]\n",
    "        eeg = sample['raw']\n",
    "        text = sample['text'][:50] + '...' if len(sample['text']) > 50 else sample['text']\n",
    "        \n",
    "        # Plot subset of channels\n",
    "        n_channels_to_plot = 10\n",
    "        for ch in range(0, 105, 105 // n_channels_to_plot):\n",
    "            ax.plot(eeg[ch, :500], alpha=0.7, linewidth=0.5)\n",
    "        \n",
    "        ax.set_xlabel('Time (samples)')\n",
    "        ax.set_ylabel('Amplitude')\n",
    "        ax.set_title(f'EEG Signal - \"{text}\"')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EEG time length distribution\n",
    "eeg_lengths = {'ZuCo 1.0': [], 'ZuCo 2.0': []}\n",
    "\n",
    "for data in zuco1_data.values():\n",
    "    for sentences in data.values():\n",
    "        for sent in sentences:\n",
    "            if sent is None:\n",
    "                continue\n",
    "            eeg_data = sent.get('sentence_level_EEG', {})\n",
    "            if 'rawData' in eeg_data:\n",
    "                raw = np.array(eeg_data['rawData'])\n",
    "                if raw.ndim == 2 and raw.shape[0] == 105:\n",
    "                    eeg_lengths['ZuCo 1.0'].append(raw.shape[1])\n",
    "\n",
    "for data in zuco2_data.values():\n",
    "    for sentences in data.values():\n",
    "        for sent in sentences:\n",
    "            if sent is None:\n",
    "                continue\n",
    "            eeg_data = sent.get('sentence_level_EEG', {})\n",
    "            if 'rawData' in eeg_data:\n",
    "                raw = np.array(eeg_data['rawData'])\n",
    "                if raw.ndim == 2 and raw.shape[0] == 105:\n",
    "                    eeg_lengths['ZuCo 2.0'].append(raw.shape[1])\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "for idx, (name, lengths) in enumerate(eeg_lengths.items()):\n",
    "    if not lengths:\n",
    "        continue\n",
    "    ax = axes[idx]\n",
    "    ax.hist(lengths, bins=50, alpha=0.7, color='seagreen', edgecolor='black')\n",
    "    ax.axvline(np.mean(lengths), color='red', linestyle='--', label=f'Mean: {np.mean(lengths):.0f}')\n",
    "    ax.axvline(500, color='purple', linestyle=':', label='Max (500)')\n",
    "    ax.set_xlabel('Time Samples')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title(f'{name}: EEG Sequence Length')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print stats\n",
    "for name, lengths in eeg_lengths.items():\n",
    "    if lengths:\n",
    "        print(f\"\\n{name} EEG Length Stats:\")\n",
    "        print(f\"  Min: {min(lengths)} samples\")\n",
    "        print(f\"  Max: {max(lengths)} samples\")\n",
    "        print(f\"  Mean: {np.mean(lengths):.0f} samples\")\n",
    "        print(f\"  % < 500: {100 * sum(1 for l in lengths if l < 500) / len(lengths):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Channel Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brain region mapping\n",
    "BRAIN_REGIONS = {\n",
    "    'Prefrontal L': list(range(0, 11)),\n",
    "    'Prefrontal R': list(range(11, 22)),\n",
    "    'Frontal L': list(range(22, 33)),\n",
    "    'Frontal R': list(range(33, 44)),\n",
    "    'Central L': list(range(44, 55)),\n",
    "    'Central R': list(range(55, 66)),\n",
    "    'Temporal L': list(range(66, 77)),\n",
    "    'Temporal R': list(range(77, 88)),\n",
    "    'Parietal-Occipital L': list(range(88, 97)),\n",
    "    'Parietal-Occipital R': list(range(97, 105)),\n",
    "}\n",
    "\n",
    "print(\"Brain Region Channel Mapping:\")\n",
    "for region, channels in BRAIN_REGIONS.items():\n",
    "    print(f\"  {region}: {len(channels)} channels ({channels[0]}-{channels[-1]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze channel statistics from samples\n",
    "if sample_data:\n",
    "    eeg = sample_data[0]['raw']\n",
    "    \n",
    "    # Per-channel statistics\n",
    "    channel_means = np.mean(eeg, axis=1)\n",
    "    channel_stds = np.std(eeg, axis=1)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "    \n",
    "    # Mean per channel\n",
    "    ax = axes[0]\n",
    "    colors = []\n",
    "    for ch in range(105):\n",
    "        for region, channels in BRAIN_REGIONS.items():\n",
    "            if ch in channels:\n",
    "                colors.append(list(BRAIN_REGIONS.keys()).index(region))\n",
    "                break\n",
    "    ax.bar(range(105), channel_means, color=plt.cm.tab10(np.array(colors) % 10), alpha=0.7)\n",
    "    ax.set_xlabel('Channel')\n",
    "    ax.set_ylabel('Mean Amplitude')\n",
    "    ax.set_title('Mean Amplitude per Channel')\n",
    "    \n",
    "    # Std per channel\n",
    "    ax = axes[1]\n",
    "    ax.bar(range(105), channel_stds, color=plt.cm.tab10(np.array(colors) % 10), alpha=0.7)\n",
    "    ax.set_xlabel('Channel')\n",
    "    ax.set_ylabel('Std Amplitude')\n",
    "    ax.set_title('Standard Deviation per Channel')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-region average power\n",
    "if sample_data:\n",
    "    eeg = sample_data[0]['raw']\n",
    "    \n",
    "    region_powers = []\n",
    "    region_names = []\n",
    "    for region, channels in BRAIN_REGIONS.items():\n",
    "        power = np.mean(np.abs(eeg[channels, :]))\n",
    "        region_powers.append(power)\n",
    "        region_names.append(region)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    bars = ax.barh(region_names, region_powers, color=plt.cm.viridis(np.linspace(0.2, 0.8, len(region_names))))\n",
    "    ax.set_xlabel('Average Power')\n",
    "    ax.set_title('Average EEG Power by Brain Region')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Processed Data Analysis (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check for processed data\n",
    "processed_dir = Path('./data/processed')\n",
    "if processed_dir.exists():\n",
    "    print(\"Processed data files:\")\n",
    "    for f in processed_dir.glob('*.pt'):\n",
    "        data = torch.load(f, weights_only=False)\n",
    "        if 'eeg' in data:\n",
    "            print(f\"  {f.name}: {data['eeg'].shape[0]} samples, EEG shape: {data['eeg'].shape[1:]}\")\n",
    "        else:\n",
    "            print(f\"  {f.name}: {list(data.keys())}\")\n",
    "else:\n",
    "    print(\"No processed data found. Run prepare_pytorch_data.py first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and visualize processed data\n",
    "if processed_dir.exists() and (processed_dir / 'train_data.pt').exists():\n",
    "    train_data = torch.load(processed_dir / 'train_data.pt', weights_only=False)\n",
    "    val_data = torch.load(processed_dir / 'val_data.pt', weights_only=False)\n",
    "    test_data = torch.load(processed_dir / 'test_data.pt', weights_only=False)\n",
    "    \n",
    "    print(\"\\nProcessed Dataset Splits:\")\n",
    "    print(f\"  Train: {train_data['eeg'].shape[0]} samples\")\n",
    "    print(f\"  Val: {val_data['eeg'].shape[0]} samples\")\n",
    "    print(f\"  Test: {test_data['eeg'].shape[0]} samples\")\n",
    "    print(f\"  Total: {train_data['eeg'].shape[0] + val_data['eeg'].shape[0] + test_data['eeg'].shape[0]} samples\")\n",
    "    print(f\"  EEG shape: {train_data['eeg'].shape[1:]}\")\n",
    "    \n",
    "    # Subject distribution\n",
    "    train_subjects = Counter(train_data['subjects'])\n",
    "    val_subjects = Counter(val_data['subjects'])\n",
    "    test_subjects = Counter(test_data['subjects'])\n",
    "    \n",
    "    print(f\"\\nTrain subjects: {list(train_subjects.keys())}\")\n",
    "    print(f\"Val subjects: {list(val_subjects.keys())}\")\n",
    "    print(f\"Test subjects: {list(test_subjects.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nðŸ“Š Dataset Statistics:\")\n",
    "total_sentences = sum(len(t) for t in all_texts.values())\n",
    "print(f\"  Total sentences: {total_sentences:,}\")\n",
    "print(f\"  Total subjects: {len(zuco1_subjects | zuco2_subjects)}\")\n",
    "print(f\"  ZuCo 1.0 subjects: {len(zuco1_subjects)}\")\n",
    "print(f\"  ZuCo 2.0 subjects: {len(zuco2_subjects)}\")\n",
    "\n",
    "print(\"\\nðŸ“ˆ EEG Data:\")\n",
    "print(f\"  Channels: 105\")\n",
    "print(f\"  Time samples (spectro): variable, padded/cropped to 500\")\n",
    "print(f\"  Brain regions: 10 (5 left, 5 right)\")\n",
    "\n",
    "print(\"\\nðŸ“ Text Statistics:\")\n",
    "all_word_counts = [len(t.split()) for texts in all_texts.values() for t in texts]\n",
    "print(f\"  Avg words/sentence: {np.mean(all_word_counts):.1f}\")\n",
    "print(f\"  Max words: {max(all_word_counts)}\")\n",
    "print(f\"  Min words: {min(all_word_counts)}\")\n",
    "\n",
    "print(\"\\nâœ… EDA Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
